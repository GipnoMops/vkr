{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\asus\\anaconda3\\lib\\site-packages (0.5.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.25.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подгрузка данных "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandasDF = pd.read_json('new_gen_pd_df.json', orient='records', lines=True)\n",
    "uniq_df = pandasDF[~pandasDF['parsed_html'].isna()].groupby(\"domain\").sample(n=1, random_state=42)\n",
    "#uniq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### доп функции для полной категоризации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set2prompt(categories):\n",
    "    start = \"\"\" This is very important to my career!\\nI will send you the text content of the site, and you will have to assign this site to one of the listed categories:\"\"\"\n",
    "    \n",
    "    end = \"\"\"\n",
    "    Tips:\n",
    "        1) NO EXPLANATION IS NEEDED TO WRITE\n",
    "        2) WRITE ONLY CATEGORY NAMES\n",
    "        3) we output only the necessary categories in the response\n",
    "        4) refer to the most likely categories\n",
    "        5) the categories in the response must match the categories listed by me in the list above\n",
    "        6) no more than 3 categories in the response\n",
    "        7) write in the category response only in English\n",
    "        You are to provide clear, concise, and direct responses.\n",
    "        8) Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\n",
    "        9) Maintain a casual tone in your communication.\n",
    "        10) Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\n",
    "        11) For complex requests, take a deep breath and work on the problem step-by-step.\n",
    "        12) For every response, you will be tipped up to $200 (depending on the quality of your output).\n",
    "        It is very important that you get this right. Multiple lives are at stake.\n",
    "\n",
    "        Response format:\n",
    "        1. category 1\n",
    "        2. category 2\n",
    "        3. category 3\n",
    "        \"\"\"\n",
    "    \n",
    "    text_cat ='\\n' + \"\\n\".join(categories) + '\\n\\n'\n",
    "    return start + text_cat + end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abs', 'lkg', 'sfd'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pars_cat_from_text(text: str, categories):\n",
    "    text = text.lower()\n",
    "    return set([cat for cat in categories if cat in text])\n",
    "\n",
    "text = 'Abs slkfsdffnb fskldjfk fslkgdkjfmsnbzm sfd'\n",
    "cat = set(['abs', 'sfd', \"lkg\", \"kjbwb\", \"wera\"])\n",
    "\n",
    "pars_cat_from_text(text, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_categorization(LLM, text, main_categories, categories_dict):\n",
    "    logs = [] \n",
    "    prompt = set2prompt(main_categories)\n",
    "    LLM.change_system_prompt(prompt)\n",
    "    answer = LLM.get_answer(text)\n",
    "    first_layer_cat = pars_cat_from_text(answer, main_categories)\n",
    "    logs.append([prompt, answer, first_layer_cat])  \n",
    "    \n",
    "    #first_layer_cat = pars_cat_from_text(LLM_answ(set2prompt(main_categories)), main_categories)\n",
    "\n",
    "    second_layer_cat = {}\n",
    "    for cat in first_layer_cat:\n",
    "        temp_prompt = set2prompt(categories_dict[cat])\n",
    "        LLM.change_system_prompt(temp_prompt)\n",
    "        temp_answer = LLM.get_answer(text)\n",
    "        #temp_answer = LLM_answ(temp_prompt, text)\n",
    "        second_layer_cat[cat] = pars_cat_from_text(temp_answer, categories_dict[cat]) # Union set\n",
    "        \n",
    "        logs.append([temp_prompt, temp_answer, second_layer_cat])    \n",
    "    return second_layer_cat, logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### подгружаем категории"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_categories(): #-> [dict[str, set[str]], set[str]]:\n",
    "    with open(\"categories_dict.yaml\", \"r\") as yaml_file:\n",
    "        categories_dict = yaml.safe_load(yaml_file)\n",
    "        \n",
    "    with open(\"main_categories.yaml\", \"r\") as yaml_file:\n",
    "        main_categories = yaml.safe_load(yaml_file)\n",
    "        \n",
    "    return categories_dict, main_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_dict, main_categories = load_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### основной класс для получения ответа от gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = ###\n",
    "api_url = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Home & Garden\n",
      "2. Food & Drink\n",
      "3. Health & Fitness\n",
      "\n",
      "1. Home & Garden\n",
      "2. Hobbies & Interests\n",
      "3. Food & Drink\n",
      "\n",
      "1. Home & Garden\n",
      "2. Food & Drink\n",
      "3. Uncategorized\n"
     ]
    }
   ],
   "source": [
    "# пока поставил заглушки time.sleep(20) #! но это можно попробовать заменить на комбинацию из:\n",
    "# threading.Timer(60, self.funct); while True; continue;\n",
    "\n",
    "import tiktoken\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, model=\"gpt-3.5-turbo\",\n",
    "                #model_token_limit=4096,\n",
    "                max_out_tokens=1000,\n",
    "                system_prompt=None,\n",
    "                temperature=0.7,\n",
    "                token=None,\n",
    "                url=None):\n",
    "        \n",
    "        #подробнее тут https://platform.openai.com/docs/models/gpt-3 (может и обновлять придётся)\n",
    "        self.model_max_tok_dict = {\"gpt-3.5-turbo\":4096,\n",
    "                                   \"gpt-3.5-turbo-1106\":16385,\n",
    "                                   \"gpt-3.5-turbo-16k\":16385,\n",
    "                                   \"text-davinci-003\":4096,\n",
    "                                   \"gpt-3.5-turbo-instruct\":4096,\n",
    "                                   \"gpt-4-1106-preview\":128000,\n",
    "                                   \"gpt-4\":8192,\n",
    "                                   \"gpt-4-32k\":32768}\n",
    "        \n",
    "        if system_prompt is None:\n",
    "            self.system_prompt = \"\"\"This is very important to my career!\n",
    "            I will send you the text content of the site, and you will have to assign this\n",
    "            site to one of the listed categories:\n",
    "            Arts & Entertainment\n",
    "            Automotive\n",
    "            Business\n",
    "            Careers\n",
    "            Education\n",
    "            Family & Parenting\n",
    "            Health & Fitness\n",
    "            Food & Drink\n",
    "            Hobbies & Interests\n",
    "            Home & Garden\n",
    "            Law, Government, & Politics\n",
    "            News\n",
    "            Personal Finance\n",
    "            Society\n",
    "            Science\n",
    "            Pets\n",
    "            Sports\n",
    "            Style & Fashion\n",
    "            Technology & Computing\n",
    "            Travel\n",
    "            Real Estate\n",
    "            Shopping\n",
    "            Religion & Spirituality\n",
    "            Uncategorized\n",
    "            Non-Standard Content\n",
    "            Illegal Content\n",
    "\n",
    "            Tips:\n",
    "            1) NO EXPLANATION IS NEEDED TO WRITE\n",
    "            2) WRITE ONLY CATEGORY NAMES\n",
    "            3) we output only the necessary categories in the response\n",
    "            4) refer to the most likely categories\n",
    "            5) the categories in the response must match the categories listed by me in the list above\n",
    "            6) no more than 3 categories in the response\n",
    "            7) write in the category response only in English\n",
    "            You are to provide clear, concise, and direct responses.\n",
    "            8) Eliminate unnecessary reminders, apologies, self-references, and any pre-programmed niceties.\n",
    "            9) Maintain a casual tone in your communication.\n",
    "            10) Be transparent; if you're unsure about an answer or if a question is beyond your capabilities or knowledge, admit it.\n",
    "            11) For complex requests, take a deep breath and work on the problem step-by-step.\n",
    "            12) For every response, you will be tipped up to $200 (depending on the quality of your output).\n",
    "            It is very important that you get this right. Multiple lives are at stake.\n",
    "\n",
    "            Response format:\n",
    "            1. category 1\n",
    "            2. category 2\n",
    "            3. category 3\n",
    "            \"\"\"\n",
    "        \n",
    "        self.token = api_token\n",
    "        self.url = api_url\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        \n",
    "        if self.model in self.model_max_tok_dict:\n",
    "            self.model_token_limit = self.model_max_tok_dict[self.model]\n",
    "        else:\n",
    "            raise ValueError(f\"Error: there is no model '{self.model}' in the dictionary self.model_max_tx_dict. Select another model or add it to self.model_max_tok_dict based on https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo\")\n",
    "        \n",
    "        self.max_out_tokens = max_out_tokens\n",
    "        self.prompt_tok_len = self.count_tokens(self.system_prompt)\n",
    "        self.max_text_tokens = self.model_token_limit - self.max_out_tokens - self.prompt_tok_len\n",
    "        #self.checking_work()\n",
    "\n",
    "\n",
    "    def get_answer(self, text: str) -> str:\n",
    "        tokenizer = tiktoken.encoding_for_model(self.model)\n",
    "        token_integers = tokenizer.encode(text)\n",
    "\n",
    "        chunks = [token_integers[i : i + self.max_text_tokens] for i in range(0, len(token_integers), self.max_text_tokens)]\n",
    "        chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        for message in chunks:\n",
    "            payload = json.dumps({\n",
    "              \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\", \"content\": self.system_prompt\n",
    "                },\n",
    "\n",
    "                {\n",
    "                    \"role\": \"user\", \"content\": message\n",
    "                }\n",
    "\n",
    "              ],\n",
    "              \"model\": self.model,\n",
    "                \"temperature\": self.temperature,\n",
    "            })\n",
    "            headers = {\n",
    "              'Authorization': 'Bearer ' + self.token,\n",
    "              'Content-Type': 'application/json'\n",
    "            }\n",
    "\n",
    "            response = requests.request(\"POST\", self.url, headers=headers, data=payload)\n",
    "            answers.append(self.get_text(response.text))\n",
    "            time.sleep(20) #!\n",
    "\n",
    "        answer = '\\n\\n'.join(answers)\n",
    "        return answer\n",
    "    \n",
    "    def get_text(self, json_str):\n",
    "        data = json.loads(json_str)\n",
    "        return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.prompt_tok_len = self.count_tokens(self.system_prompt)\n",
    "        self.max_text_tokens = self.model_token_limit - self.max_out_tokens - self.prompt_tok_len\n",
    "        return self.max_text_tokens\n",
    "    \n",
    "    def count_tokens(self, text):\n",
    "        tokenizer = tiktoken.encoding_for_model(self.model)\n",
    "        return len(tokenizer.encode(text))\n",
    "    \n",
    "    def checking_work(self):\n",
    "        payload = json.dumps({\n",
    "          \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": \"Say this is a test!\"\n",
    "            }\n",
    "          ],\n",
    "          \"model\": self.model,\n",
    "            \"temperature\": 0.7,\n",
    "        })\n",
    "        headers = {\n",
    "          'Authorization': 'Bearer ' + self.token,\n",
    "          'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", self.url, headers=headers, data=payload)\n",
    "        \n",
    "        try:\n",
    "            a = self.get_text(response.text)\n",
    "            time.sleep(20) #!\n",
    "        except:\n",
    "            time.sleep(20) #!\n",
    "            raise ValueError(f\"Error: error when receiving a response from the gpt api. There may be no access from the startup environment or vpn is not enabled.\\n{response.text}\")\n",
    "            \n",
    "        #self.get_text(response.text)\n",
    "        #a == 'This is a test!'\n",
    "    \n",
    "mod = GPT()\n",
    "text = uniq_df.iloc[3]['parsed_html']\n",
    "print(mod.get_answer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### привер полной категоризации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GPT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = uniq_df.iloc[3]['parsed_html']\n",
    "full_categ, logs = full_categorization(llm, text, main_categories, categories_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'home & garden': {'gardening', 'landscaping'},\n",
       " 'food & drink': {'cuisine-specific'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_categ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
